## S06: S3

#### Summary

- Buckets vs Objects
  - buckets: global unique name, tied to a region
  - objects: go in buckets
- **S3 security**: IAM policy S3 Bucket Policy (public access), S3 Encryption
- **S3 websites**: host a static website on Amazon S3
- **S3 Versioning**: multiple versions for files, prevent accidental deletes
- **S3 Access Logs**: log requests made within your S3 bucket
- **S3 Replication**: same-region or cross-region must enable versioning
- **S3 Storage Classes**: Standard, IA, IZ-IA, Intelligent, Glacier, Glacier Deep Archive
- **S3 Lifecycle Rules**: transition objects between classes
- **S3 Glacier Vault Lock / S3 Object Lock**: WORM (write once read many)
- **Snow Family**: import data onto S3 through a physical device, edge computing
- **OpsHub**: desktop application to manage Snow Family devices
- **Storage Gateway**: hybrid solution to extend on-premises storage to S3



- "infinitely scaling" storage
- Many websites use Amazon S3 as backbone
- AWS services uses S3 as integration as well



#### Use Cases

- Backup and storage
- Disaster Recovery
- Archive
- Hybrid Cloud storage
- Application hosting
- Media hosting
- Data lakes & big data analytics
- Software delivery
- Static website



#### Buckets

- Allows storage of objects (files) in "buckets" (directories)
- Buckets must have a **globally unique name** (across all regions all accounts)
- Buckets are defined at the region level
- S3 looks like a global service, but buckets are created in a region



#### Objects

- Objects (files) have a Key
- Key is full path
  - s3://my-bucket/**my_file.txt**
  - s3://my-bucket/**my_folder1/another_folder/my_file.txt**
- Key is composed of prefix + object name
- No concept of directories within buckets (UI will trick you)
- Object values are the content of the body
  - Max Object Size is 5TB (5000 GB)
  - If uploading more than 5GB, must use **multi-part-upload**
- Metadata (list of text key / value pairs - system or user metadata)
- Tags (Unicode key / value pair - up to 10) - for security / lifecycle
- Version ID (if versioning is enabled)



#### S3 Security

- User based
  - IAM policies - which API calls should be allowed for a specific userfrom IAM console
- Resource Based
  - Bucket Policies - bucket wide rules from the S3 console - allows cross account 
  - Object Access Control List (ACL) - finer grain
  - Bucket Access Control List (ACL) - less common
- Note: an IAM principal can access an S3 object if
  - User IAM permissions allow it OR the resource policy allows it
  - AND no explicity DENY
- Encryption: encrypt objects in Amazon S3 using encryption keys

Examples

- Public Access - Use Bucket Policy
- User Access to S3 - IAM permissions
- EC2 instance access - use IAM Roles
- Advanced: Cross-Account Access - Use Bucket Policy



S3 Bucket Policies

- JSON based policies
- Resources: buckets and objects
- Actions: Set of API to Allow or Deny
- Effect: Allow / Deny
- Principal: The account or user to apply the policy to

- Use cases
  - Grant public access to the bucket
  - Force objects to be encrypted at upload
  - Grant access to another account (Cross Account)

Bucket settings for Block Public Access

- Settings were created to prevent company data leaks
- Can be set at account level
- the checkboxes you see in the UI



#### S3 Websites

- S3 can host static websites and have them accessible on the www
- The website URL either be:
  - <bucket-name>-s3-website-<AWS-region>.amazonaws.com
  - <bucket-name>-s3-website.<AWS-region>.amazonaws.com
- 403 - make sure bucket policy allows public access



#### S3 - Versioning

- Version your files
- Enabled at the bucket level
- Same key overwrite will increment the "version": 1,2,3
- It is best practice to version buckets
  - Protect against unintended deletes (ability to restore a version)
  - Easy roll back to previous version
- Notes
  - File not versioned prior will have null version
  - Suspending versioning does not delete previous versions



#### S3 Access Logs

- For audit purpose, may log all access to S3
- Any request made to S3 from any account, authorized + denied will be logged into another S3 bucket
- Data can be analyzed
- Use cases: View Suspicious patterns, audit usage etc.



#### S3 Replication

- Must **enable versioning in source and destinations**
- Cross Region Replication (CRR)
  - Use cases: Compliance, lower latency access, replication across accounts
- Same Region Replication (SRR)
  - Use Cases: log aggregation, live replication between production and test accounts
- Can replicate across regions or same regions (can be in different accounts)
- Copying is asynchronous
- Must give proper IAM permissions to S3
- Objects before enabling replication are not replicated (need to re-upload)
- Same version ID on replication



#### S3 Durability and Availablity

- Durability
  - High durability (11 9's) of objects across multiple AZ
  - Same for all storage classes
- Availability
  - Measures how readily available a service is
  - S3 standard has 99.99% availability
  - Varies depending on storage class

#### S3 Storage Classes

- **Amazon S3 Standard - General Purpose**
  - 99.99% availability
  - Used for frequently access data
  - Low latency and high throughput
  - Sustain 2 concurrent facility failure
  - Use cases
    - Big Data analytics
    - Mobile + gaming applications
    - Content distribution

- **Amazon S3 Standard-Infrequent Access (IA)**
  - Data less frequently accessed, but requires rapid access when needed
  - 99.9% Availability
  - Lower cost compared to S3 Standard, but fee for retrieval
  - Sustain 2 concurrent faciltiy failures
  - Use Cases
    - Data store disaster recovery
    - Backups
- **Amazon S3 One Zone-Infrequent Access**
  - Same as above (except only one AZ)
  - 99.5% availability
  - Low latency and high throughput performance
  - Lower cost compared to S3-IA (by 20%)
  - Use Cases
    - Storing secondary backup copies of on-premise data
    - storing data you can recreate
- **Amazon S3 Intelligent Tiering**
  - 99.9% availability
  - Same low latency and high throughput performance of S3 Standard
  - Cost-optimized by automatically moving objects between two access tiers based on changing access patterns
    - Frequent access
    - Infrequent access
- **Amazon Glacier**
  - Resilient against events that impact an entire AZ
  - Low cost object storage (GB / month) meant for archiving / backup
  - Data is retained for the longer term (years)
  - Various retrieval options of time + fees for retrieval
  - Amazon Glacier - cheap
    - Expedited (1 - 5 minutes)
    - Standard (3 - 5 hours)
    - Bulk (5 - 12 hours)
- **Amazon Glacier Deep Archive**
  - Cheapest
    - Standard (12 hours)
    - Bulk (48 hours)

- Amazon S3 Reduced Redundancy Storage (deprecated - omitted)



Note: You can transition objects between storage classes

- For infrequent accessed, move to Standard IA
- For archive objects: Glacier or deep archive
- Moving objects can be automated using a lifecycle configuration



#### S3 Object Lock + Glacier Vault Lock

- S3 Object Lock
  - Adopt a WORM (Write Once Read Many) model
  - Block an object version deletion for a specified amount of time
  - Cannot modify either
- Glacier Vault Lock
  - Adopt a WORM (Write Once Read Many) model
  - Lock the policy for future edits (no longer be changed once set)
  - Helpful for compliance and data retention
- Applies to admins as well!



#### Shared Responsibility

- AWS
  - Infrastructure (global security, durability, availability, sustain concurrent loss of data in two facilities)
  - Configuration and vulnerability analysis
  - Compliance validation
- User
  - S3 Versioning
  - S3 Bucket Policies
  - S3 Replication Setup
  - Logging and Monitoring
  - S3 Storage Classes
  - Data encryption at rest and in transit



#### AWS Snow Family

- Highly secure, portable devices to collect and process data at the edge and migrate data into and out of AWS
- Data migrations with Snow Family
  - 100 TB could take 12 days to migrate (with 1gbps)
  - Challenges
    - Limited connectivity
    - Limited bandwidth
    - High network cost
    - Shared bandwidth (can't max out the line)
    - Connection stability
  - If it takes more than a week to transfer over the network, use Snowball devices!
- With Snow Family
  - client get physical device through mail
  - Loads data onto device
  - Ships device to AWS
  - And AWS imports into S3 bucket
- **Devices for Data Migration**
  - Snowball Edge (for data transfers)
    - Physical data transport solution: move TBs or PBs of data in or out of AWS
    - Alternative to moving data over network
    - Pay per data transfer job
    - Provide block storage and S3-compatible object storage
    - Edge Storage Optimized
      - 80 TB HDD capacity 
    - Edge Compute Optimized
      - 42 TB of HDD capacity
    - Use Cases
      - large data cloud migration
      - DC decommission 
      - disaster recovery
  - Snowcone (data migration)
    - Smaller than Snowball Edge
    - Small, portable computing anywhere rugged & secure, withstands harsh environments
    - Light (4.5 pounds)
    - 8 TBs of usable storage
    - Must provide own battery + cables
    - Use snowcone where Snowball does not fit (space-constrained)
    - Can be set back to AWS offline or use AWS DataSync to send data
  - Snowmobile (data migration)
    - Transfer exabytes of data (1 EB = 1000 PB = 1,000,000 TBs)
    - Each snowmobile has 100 PB of capacity
    - High Security: temperature controlled, GPS, 24/7 surveillance
    - It's a truck!

![Screen Shot 2022-04-01 at 1.54.57 AM](/Users/sriharivishnu/Documents/Notes/AWS/CLF-C01/S06_S3-images/Screen Shot 2022-04-01 at 1.54.57 AM.png)

Process:

1. Request Snowball devices from AWS console for delivery
2. Install snowball client / AWS OpsHub on servers
3. Connect the snowball to servers and copy files using client
4. Ship back device
5. Data loaded into S3
6. Snowball wiped



- **Devices for Edge Computing**
  - Process data while it's being created on an edge location
    - Truck on the road, a ship on the sea, mining station underground
  - Locations
    - Limited / no internet access
    - Limited / no easy access to computing power
  - Set up Snowball edge / Snowcone device to do edge computing
  - Use cases
    - Preprocess data
    - Machine learning at the edge
    - Transcoding media streams
  - Can be shipped back to AWS eventually
  - Devices
    - Snowcone (smaller)
      - 2 CPUs, 4 GB memory, wired or wireless
      - USB-C power + battery
    - Snowball Edge - Compute Optimized
      - 52 vCPUs, 208 GiB RAM
      - Optional GPU
      - 42 TB storage
    - Snowball Edge - Storage Optimized
      - 40 vCPUs, 80 GiB of RAM
    - All: can run EC2 instances + AWS lambda functions
    - Long-term deployment options: 1 and 3 years discounted pricing
- AWS OpsHub
  - Before needed CLI to use snow family devices
  - Today, you can use AWS OpsHub (can install on computer + laptop)
    - Unlocking and configuring single or clustered devices
    - transferring files
    - Launcing and managing instances running on Snow Family Devices
    - Monitor device metrics
    - Launch compatible AWS services on your devices (e.g. EC2 instances, DataSync, NFS)



#### Storage Gateway

- AWS is pushing for "hybrid cloud"
  - Part on-premises
  - Part on cloud
- Can be due to 
  - Long cloud migrations
  - Security requirements
  - Compliance requirements
  - IT strategy
- S3 is proprietary storage technology (how to expose the S3 data?)
- Storage gateway is a bridge between on-premise data and cloud data in s3
- Hybrid storage service to allow on-premises to seamlessly use the AWS Cloud
- Types
  - File Gateway
  - Volume Gateway
  - Tape Gateway